# Bitcoin_price_prediction_lstm-savitzky-golay
The interest in cryptocurrencies is increasing among individuals and investors. Bitcoin is the leading existing cryptocurrency with the highest market capitalization. However, its high volatility aligns with political uncertainty making it very difficult to predict its value. Therefore, there is a need to create advanced models that use mathematical and statistical methods to reduce investment risk. This research aims to verify if long short-term memory (LSTM), and bidirectional long short-term memory (BiLSTM) neural networks, can be used with Savitzky–Golay filter to predict next-day bitcoin closing prices. We found evidence both networks can be used effectively to predict bitcoin prices. LSTM performed 4.49 mean absolute percentage error (MAPE) and BiLSTM 4.44 MAPE. We also found that using Savitzky–Golay filter and dropout regularization significantly improved the model’s prediction performance.

Keywords: forecasting; cryptocurrency; Savitzky–Golay; LSTM; BiLSTM; neural networks

# Methodology strategy
![image](https://github.com/pereirajose3/bitcoin-price-prediction-ensemble-bilstm-gru-savitzky-gola/assets/37916212/c59c6864-2462-4e83-b164-c9e65da3ea87)

Our methodology follows the deep learning system architecture presented in figure 3.1. Deep learning is a subfield of machine learning and aims to mimic how humans gain a specific type of knowledge through experiences. The word ‘Deep’ represents using a neural network with more than three layers of depth (Chollet, 2021). The network depth creates a deep hierarchical representation learning, where layers are stacked on top of each other. It is a multistage information distillation process where the information is purified by passing through several filters. The network learns data representations through the multistage sequence process. As shown in figure 3.1, the methodology was structured following a sequence of processes. First, economic variables and bitcoin prices were collected. Second, feature selection was performed with the wrapper forward selection method. Third, the volatility and the noise of the bitcoin closing prices were removed using a Savitzky–Golay filter. Fourth, data pre-processing was performed to prepare the data for deep learning algorithms. The data set was divided into three chunks: 65% for training, 15% for validation, and 20% for testing. The normalization of the three sets was performed to have the data on the same scale. In order to have the data ready for the deep learning algorithms, a rank-3 tensor was used to create sequences of 16-time steps as input and 1-time step as the label. Savitzky–Golay filter was only applied to training labels. Selecting the right architecture for deep learning systems is very important. In this research study, two types of networks were used, Long Short-term Memory neural networks (LSTM) and Bi-Directional Long Short-term Memory neural networks (Bi-LSTM), variants of Recurrent Neural Networks (RNN), a type of neural network well-suited to process time series step-by-step. The network learning process was accomplished by observing and mapping a significant number of inputs and labels through a deep sequence of data transformations (layers) (Chollet, 2021). The transformation done on the inputs was performed by the layer weights, which are also called layer parameters (illustrated in figure 3.1). The learning process consisted of finding the layers weights values that allowed the network to map the inputs and their associated labels correctly. A network can contain many layers; therefore, finding the correct value for all the weights is a complex task. To control the output of the LSTM network – the algorithm first had to observe and measure how far the output (prediction) was from the actual value (Chollet, 2021). The measurement was performed using the MSE loss function, which compares the distance (loss score) between the forecasts and the true value. The loss score was used as a response signal to adjust the values of the weights in a direction that allowed the algorithm to minimize the loss score. The adjustment was made by the optimizer, using a gradient descendent algorithm. The gradient of the loss regarding the model’s parameters is computed to find the downhill direction, and the weights (parameters) are moved in small steps (equation 3.1) in the opposite direction from the gradient (equation 3.2), allowing to reduce the loss a little each iteration. Figure 3.2 shows how the optimizer works. The weights (w) are randomly initiated and repetitively adjusted with small steps until the algorithm converges to a value close to the global minimum. This is achieved using the learning rate hyperparameter and the loss gradient. The learning rate controls the speed of the gradient descendent; therefore, it is crucial to choose a reasonable value for this hyperparameter. If the learning rate is too low, the algorithm will have to go through many iterations, and the loss value may get stuck in a local minimum. If the learning rate is too large, the loss value may exceed the global minimum and jump between completely random locations on the loss curve.

step=learning rate*gradient(loss,W) 	(3.1)

W=W-learning rate*gradient(loss,W) 	(3.2)


![image](https://github.com/pereirajose3/bitcoin-price-prediction-ensemble-bilstm-gru-savitzky-gola/assets/37916212/44cbd920-e348-4242-b749-41c68a164259)

Figure 3.2: Weights optimizer The gradient descent algorithm measures the local gradient of the loss value concerning the weights (w), and follows the direction that allows obtaining a greater gradient descent (Géron, 2019). When the gradient is zero, the minimum has been reached. The gradient calculates how much the loss value changes when the weights are slightly tweaked. This process is performed iteratively until the minimum is found. Equation 3.3 shows how to calculate the gradient; the aim is to find the set of weights that minimizes the loss value.

∆_W MSE(W)=2/n X^T (X.W-Y) 	 (3.3)

Where W are the weights, n is the number of observations used in the batch where MSE is measured, X is the matrix containing all the features values of the batch (excluding labels), T corresponds to the transpose matrix, and Y is the matrix containing all the labels values of the batch. Once the gradient vector (∆_W MSE(W)) is obtained, it is subtracted from W, to go in the downhill direction. The gradient is multiplied by the learning rate to determine the size of the downhill step (equation 3.2). The LSTM network contains several hyper-parameters that were adjusted to improve the performance of the algorithm’s predictions. The model validation error was used as a response signal to adjust the LSTM hyper-parameters in the direction that allowed the algorithm to minimize the validation error using manual fine-tuning strategy.
